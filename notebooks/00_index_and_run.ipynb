{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Run Moatless Tools",
   "id": "7fea6d23c616b470"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "First, index the codebase in a vector store.\n",
    "\n",
    "Set `repo_dir` to the path of the repository you want to index."
   ],
   "id": "69c80c1cba7d5c37"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T05:14:44.351498Z",
     "start_time": "2024-06-17T05:14:22.983524Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from moatless.index import CodeIndex, IndexSettings\n",
    "from moatless import FileRepository, Workspace\n",
    "\n",
    "# An OPENAI_API_KEY is required to use the OpenAI Models\n",
    "model = \"gpt-4o-2024-05-13\"\n",
    "index_settings = IndexSettings(\n",
    "    embed_model=\"text-embedding-3-small\"\n",
    ")\n",
    "\n",
    "repo_dir = \"/tmp/moatless-tools\"\n",
    "file_repo = FileRepository(repo_path=repo_dir)\n",
    "\n",
    "code_index = CodeIndex(file_repo=file_repo, settings=index_settings)\n",
    "nodes, tokens = code_index.run_ingestion()\n",
    "\n",
    "print(f\"Indexed {nodes} nodes and {tokens} tokens\")\n",
    "\n",
    "workspace = Workspace(file_repo=file_repo, code_index=code_index)"
   ],
   "id": "b9dd5259592cc65e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parsing nodes:   0%|          | 0/48 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d211903488e44ad5af423bb005f1076e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Generating embeddings:   0%|          | 0/198 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8369d9d013964b12bb2488adbce5cdd3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed 198 nodes and 65001 tokens\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Then use the `SearchLoop` to find the relevant code.",
   "id": "bc13c37fd492267f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T05:14:47.217658Z",
     "start_time": "2024-06-17T05:14:44.355086Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from moatless.transitions import search_transitions\n",
    "from moatless import AgenticLoop\n",
    "\n",
    "instructions = \"Remove the token limit check from the completion function\"\n",
    "\n",
    "transitions = search_transitions()\n",
    "search_loop = AgenticLoop(transitions, workspace=workspace)\n",
    "\n",
    "search_response = search_loop.run(instructions)\n",
    "print(search_response.message)\n",
    "\n",
    "print(workspace.file_context.create_prompt())"
   ],
   "id": "38c6f7f6422053fb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The relevant code span for the completion function is found in moatless/llm/completion.py. The token limit check is present in the line `if tokens > Settings.max_message_tokens:`. This is the code that needs to be modified to remove the token limit check.\n",
      "\n",
      "\n",
      "moatless/llm/completion.py\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "def completion(\n",
      "    model: str,\n",
      "    messages: List,\n",
      "    max_tokens: int = 1000,\n",
      "    temperature: float = 0.0,\n",
      "    trace_name: str = \"moatless-agent\",\n",
      "    stop: Optional[List[str]] = None,\n",
      "    generation_name: Optional[str] = None,\n",
      "    tools: Optional[List[Dict[str, Any]]] = None,\n",
      ") -> litellm.ModelResponse:\n",
      "    if len(messages) == 0:\n",
      "        raise ValueError(\"At least one message is required.\")\n",
      "\n",
      "    global _trace_metadata, _mock_response\n",
      "    metadata = {}\n",
      "    metadata.update(_trace_metadata)\n",
      "\n",
      "    if generation_name:\n",
      "        metadata[\"generation_name\"] = generation_name\n",
      "\n",
      "    metadata[\"trace_name\"] = trace_name\n",
      "\n",
      "    tokens = token_counter(messages=messages[-1:])\n",
      "    if tokens > Settings.max_message_tokens:\n",
      "        raise ValueError(f\"Too many tokens in the new message: {tokens}\")\n",
      "\n",
      "    response = litellm.completion(\n",
      "        model=model,\n",
      "        max_tokens=max_tokens,\n",
      "        temperature=temperature,\n",
      "        tools=tools,\n",
      "        stop=stop,\n",
      "        metadata=metadata,\n",
      "        messages=messages,\n",
      "        mock_response=_mock_response,\n",
      "    )\n",
      "\n",
      "    return response\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Execute the `CodeLoop` to apply the changes.",
   "id": "c160cc8c9c7e202e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T05:15:06.329522Z",
     "start_time": "2024-06-17T05:14:47.219845Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from moatless.transitions import code_transitions\n",
    "\n",
    "code_loop = AgenticLoop(transitions=code_transitions(), workspace=workspace)\n",
    "code_response = code_loop.run(instructions)\n",
    "\n",
    "print(f\"Response: {code_response.message}\")"
   ],
   "id": "903b67c9dff5c384",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: The token limit check has been successfully removed from the `completion` function in moatless/llm/completion.py.\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Run a `$ git diff` to see the changes.",
   "id": "5d131ca3793b26a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T05:15:06.434193Z",
     "start_time": "2024-06-17T05:15:06.332771Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import subprocess\n",
    "\n",
    "output = subprocess.run(\n",
    "      [\"git\", \"diff\"],\n",
    "      capture_output=True,\n",
    "      text=True,\n",
    "      cwd=repo_dir,\n",
    ")\n",
    "\n",
    "print(output.stdout)"
   ],
   "id": "d51ba9eceb0b7288",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diff --git a/moatless/llm/completion.py b/moatless/llm/completion.py\n",
      "index 2c95e47..a926948 100644\n",
      "--- a/moatless/llm/completion.py\n",
      "+++ b/moatless/llm/completion.py\n",
      "@@ -48,10 +48,6 @@ def completion(\n",
      " \n",
      "     metadata[\"trace_name\"] = trace_name\n",
      " \n",
      "-    tokens = token_counter(messages=messages[-1:])\n",
      "-    if tokens > Settings.max_message_tokens:\n",
      "-        raise ValueError(f\"Too many tokens in the new message: {tokens}\")\n",
      "-\n",
      "     response = litellm.completion(\n",
      "         model=model,\n",
      "         max_tokens=max_tokens,\n",
      "\n"
     ]
    }
   ],
   "execution_count": 7
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
